{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "39a6f271",
      "metadata": {
        "id": "39a6f271"
      },
      "source": [
        "edxdxdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "483059f3",
      "metadata": {
        "id": "483059f3"
      },
      "outputs": [],
      "source": [
        "# UNQ_C1\n",
        "# GRADED FUNCTION: sigmoid\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Args:\n",
        "        z (ndarray): A scalar, numpy array of any size.\n",
        "\n",
        "    Returns:\n",
        "        g (ndarray): sigmoid(z), with the same shape as z\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    g = 1 / (1 + np.exp(-z))\n",
        "    ### END SOLUTION ###\n",
        "\n",
        "    return g\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb186d8",
      "metadata": {
        "id": "dbb186d8"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "`# This is formatted as code`\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def compute_cost(X, y, w, b, *args):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "        X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "        y : (ndarray Shape (m,)) target value\n",
        "        w : (ndarray Shape (n,)) values of parameters of the model\n",
        "        b : (scalar) value of bias parameter of the model\n",
        "        *args : unused\n",
        "    Returns:\n",
        "        total_cost : (scalar) cost\n",
        "    \"\"\"\n",
        "\n",
        "    m, n = X.shape\n",
        "    loss_sum = 0\n",
        "\n",
        "    for i in range(m):\n",
        "        z_wb = 0\n",
        "\n",
        "        for j in range(n):\n",
        "            z_wb += w[j] * X[i][j]   # z = w·x\n",
        "\n",
        "        z_wb += b                    # z = w·x + b\n",
        "\n",
        "        f_wb = sigmoid(z_wb)        # prediction\n",
        "\n",
        "        loss_i = -y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb)\n",
        "\n",
        "        loss_sum += loss_i          # accumulate loss\n",
        "\n",
        "    total_cost = loss_sum / m       # average loss\n",
        "\n",
        "    return total_cost\n"
      ],
      "metadata": {
        "id": "MXQMrzw4XkKx"
      },
      "id": "MXQMrzw4XkKx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_gradient(X, y, w, b, *argv):\n",
        "    \"\"\"\n",
        "    Computes the gradient for logistic regression\n",
        "\n",
        "    Args:\n",
        "    X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "    y : (ndarray Shape (m,)) target value\n",
        "    w : (ndarray Shape (n,)) values of parameters of the model\n",
        "    b : (scalar) value of bias parameter of the model\n",
        "    *argv : unused, for compatibility with regularized version below\n",
        "\n",
        "    Returns:\n",
        "    dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.\n",
        "    dj_db : (scalar) The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    dj_dw = np.zeros((n,))                   # derivative of cost w.r.t. w\n",
        "    dj_db = 0.                               # derivative of cost w.r.t. b\n",
        "\n",
        "    for i in range(m):\n",
        "        # Calculate f_wb (exactly as you did in the compute_cost function above)\n",
        "        f_wb = 1 / (1 + np.exp(-np.dot(X[i], w) - b))  # sigmoid\n",
        "\n",
        "        # Calculate the gradient for b from this example\n",
        "        dj_db_i = f_wb - y[i]  # Your code here to calculate the error\n",
        "        dj_db += dj_db_i\n",
        "\n",
        "        # get dj_dw for each attribute\n",
        "        for j in range(n):\n",
        "            # Your code here to calculate the gradient from the i-th example for j-th attribute\n",
        "            dj_dw_i = (f_wb - y[i]) * X[i, j]\n",
        "            dj_dw[j] += dj_dw_i\n",
        "\n",
        "    # divide dj_db and dj_dw by total number of examples\n",
        "    dj_db = dj_db / m\n",
        "    dj_dw = dj_dw / m\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return dj_db, dj_dw\n",
        "\n",
        "# Test the function (example data)\n",
        "X_train = np.array([[1., 1.], [1., 2.], [1., 3.]])  # 3 examples, 2 features\n",
        "y_train = np.array([0., 0., 1.])                    # 3 labels\n",
        "initial_w = np.zeros(2)\n",
        "initial_b = 0.\n",
        "\n",
        "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
        "print(f\"dj_db at test w and b: {dj_db:.9f}\")\n",
        "print(f\"dj_dw at test w and b: {dj_dw}\")"
      ],
      "metadata": {
        "id": "DItVxrZlDcCP"
      },
      "id": "DItVxrZlDcCP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# UNQ_C4\n",
        "# GRADED FUNCTION: predict\n",
        "\n",
        "def predict(X, w, b):\n",
        "    \"\"\"\n",
        "    Predict whether the label is 0 or 1 using learned logistic\n",
        "    regression parameters w\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "\n",
        "    Returns:\n",
        "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m, n = X.shape\n",
        "    p = np.zeros(m)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Loop over each example\n",
        "    for i in range(m):\n",
        "        z_wb = 0                      # initialize linear combination\n",
        "        for j in range(n):\n",
        "            z_wb += w[j] * X[i][j]    # compute w · x\n",
        "\n",
        "        z_wb += b                     # add bias\n",
        "\n",
        "        f_wb = 1 / (1 + np.exp(-z_wb))  # sigmoid\n",
        "\n",
        "        # Apply threshold\n",
        "        if f_wb >= 0.5:\n",
        "            p[i] = 1\n",
        "        else:\n",
        "            p[i] = 0\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return p"
      ],
      "metadata": {
        "id": "v0vVshx8XnCh"
      },
      "id": "v0vVshx8XnCh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C5\n",
        "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples with regularization\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      y : (ndarray Shape (m,))  target value\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "      lambda_ : (scalar, float) Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost : (scalar)     cost\n",
        "    \"\"\"\n",
        "\n",
        "    m, n = X.shape\n",
        "\n",
        "    # Calls the compute_cost function that you implemented above\n",
        "    cost_without_reg = compute_cost(X, y, w, b)\n",
        "\n",
        "    # Regularization term\n",
        "    reg_cost = (lambda_ / (2 * m)) * np.sum(np.square(w))\n",
        "\n",
        "    # Add the regularization cost to get the total cost\n",
        "    total_cost = cost_without_reg + reg_cost\n",
        "\n",
        "    return total_cost\n"
      ],
      "metadata": {
        "id": "bU34DB0HDkYE"
      },
      "id": "bU34DB0HDkYE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_reg(X, y, w, b, lambda_ = 1):\n",
        "    m, n = X.shape\n",
        "\n",
        "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
        "\n",
        "    # تأكد أن dj_db قيمة عددية وليس مصفوفة\n",
        "    if isinstance(dj_db, np.ndarray):\n",
        "        dj_db = np.squeeze(dj_db)\n",
        "\n",
        "    # إضافة التنظيم إلى التدرجات\n",
        "    dj_dw = dj_dw + (lambda_ / m) * w\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "TrkvgxKbDlt3"
      },
      "id": "TrkvgxKbDlt3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dxdxdxd"
      ],
      "metadata": {
        "id": "jg9pER4Iu0cR"
      },
      "id": "jg9pER4Iu0cR"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}